{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-01 20:47:03--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  1.64MB/s    in 0.6s    \n",
      "\n",
      "2024-01-01 20:47:05 (1.64 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First we need to download the data \n",
    "# We will use shakespeare data\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.2-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.1.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from torchvision) (1.26.1)\n",
      "Requirement already satisfied: requests in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/beckdevil/anaconda3/envs/mldl/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp310-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.2-cp310-cp310-macosx_11_0_arm64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.1.2-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.1.2 torchaudio-2.1.2 torchvision-0.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_embed = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters : 1115394\n"
     ]
    }
   ],
   "source": [
    "# Read it in to inspect it \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f\"Length of dataset in characters : {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size is 65\n"
     ]
    }
   ],
   "source": [
    "# Getting the vocabulary- Explore all new advancements \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f\"Vocab size is {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43, 2]\n",
      "hello there!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# TODO: do the survey eg. google's sentencepiece(sub-word), openai tiktoken(byte-pair)\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder take the string, convert it to list of integers \n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder: take the list of integers, convert into a string \n",
    "\n",
    "print(encode(\"hello there!\"))\n",
    "print(decode(encode(\"hello there!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the entire dataset \n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us split the data into training and validation sets \n",
    "n = int(0.9*len(data)) # using first 90% for training \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to train the transformers,\n",
    "# we can't send the entirety of the train section at once as it would be computationally prohitive\n",
    "# we'll divide the training dataset into the chunks of inputs known as \"sequences\" or \"blocks\"\n",
    "# Think of these sequences as time dimension or sequence-length\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target: 47\n",
      "When input is tensor([18, 47]), the target: 56\n",
      "When input is tensor([18, 47, 56]), the target: 57\n",
      "When input is tensor([18, 47, 56, 57]), the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, the target: {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
      "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
      "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
      "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
      "targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
      "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
      "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
      "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
      "--------------\n",
      "When the input is [57], the output is 1\n",
      "When the input is [57, 1], the output is 46\n",
      "When the input is [57, 1, 46], the output is 47\n",
      "When the input is [57, 1, 46, 47], the output is 57\n",
      "When the input is [57, 1, 46, 47, 57], the output is 1\n",
      "When the input is [57, 1, 46, 47, 57, 1], the output is 50\n",
      "When the input is [57, 1, 46, 47, 57, 1, 50], the output is 53\n",
      "When the input is [57, 1, 46, 47, 57, 1, 50, 53], the output is 60\n",
      "When the input is [1], the output is 58\n",
      "When the input is [1, 58], the output is 46\n",
      "When the input is [1, 58, 46], the output is 43\n",
      "When the input is [1, 58, 46, 43], the output is 56\n",
      "When the input is [1, 58, 46, 43, 56], the output is 43\n",
      "When the input is [1, 58, 46, 43, 56, 43], the output is 1\n",
      "When the input is [1, 58, 46, 43, 56, 43, 1], the output is 41\n",
      "When the input is [1, 58, 46, 43, 56, 43, 1, 41], the output is 39\n",
      "When the input is [17], the output is 26\n",
      "When the input is [17, 26], the output is 15\n",
      "When the input is [17, 26, 15], the output is 17\n",
      "When the input is [17, 26, 15, 17], the output is 10\n",
      "When the input is [17, 26, 15, 17, 10], the output is 0\n",
      "When the input is [17, 26, 15, 17, 10, 0], the output is 32\n",
      "When the input is [17, 26, 15, 17, 10, 0, 32], the output is 53\n",
      "When the input is [17, 26, 15, 17, 10, 0, 32, 53], the output is 1\n",
      "When the input is [57], the output is 58\n",
      "When the input is [57, 58], the output is 6\n",
      "When the input is [57, 58, 6], the output is 1\n",
      "When the input is [57, 58, 6, 1], the output is 61\n",
      "When the input is [57, 58, 6, 1, 61], the output is 47\n",
      "When the input is [57, 58, 6, 1, 61, 47], the output is 58\n",
      "When the input is [57, 58, 6, 1, 61, 47, 58], the output is 46\n",
      "When the input is [57, 58, 6, 1, 61, 47, 58, 46], the output is 0\n"
     ]
    }
   ],
   "source": [
    "# We need batching to properly utilize the training hardware and speed up the process\n",
    "# \n",
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs X and outputs y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "    \n",
    "xb, yb = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets: ')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print (\"--------------\")\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When the input is {context.tolist()}, the output is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(torch.nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        k = self.key(x) # (B, T, 16)\n",
    "        q = self.query(x) # (B, T, 16)\n",
    "        wei = q @ k.transpose(-2, -1) * C **-0.5 # (B, T, 16) x (B, 16, T) -> (B, T, T)\n",
    "        \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        sei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        out = wei @ v\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Using multiple heads and concatenating the result \n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = torch.nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear layer followed by a simple non-linearity \n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, 4 * n_embed),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4 * n_embed, n_embed),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer decoder block\n",
    "    (Without the cross attention section obviously, because we have a decoder only network)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embed%n_head ==0, \"The embedding size must be exactly divisible by number of heads\"\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.ln2 = torch.nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65]) tensor(4.3782, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lets start with the bigram model \n",
    "import torch \n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #each token directly reads off the logits for the next token from a lookup table \n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = torch.nn.Embedding(block_size, n_embed)\n",
    "        # self.sa_head = Head(n_embed)\n",
    "        # self.sa_heads = MultiHeadAttention(4, n_embed/4) # four heads each with head_size (output) 1/4th of the embed size\n",
    "        # self.ffwd = FeedForward(n_embed)\n",
    "        self.blocks = torch.nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = torch.nn.LayerNorm(n_embed)\n",
    "        # self.blocks = torch.nn.Sequential(\n",
    "        #     Block(n_embed, n_head=4),\n",
    "        #     Block(n_embed, n_head=4),\n",
    "        #     Block(n_embed, n_head=4),\n",
    "        #     torch.nn.LayerNorm(n_embed), # there needs to be a Layernorm at the end of transformer blocks as well ?\n",
    "        # )\n",
    "\n",
    "        # final decoding head, do not touch\n",
    "        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb # (B, T, C) - broadcast happens \n",
    "        # x = self.sa_head(x)\n",
    "        # x = self.sa_heads(x)\n",
    "        # x = self.ffwd(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "    \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is the (B, T) tensor of integers \n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to last block_size tokens \n",
    "            idx_cond = idx[:,Z -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) # Logits are (B,T,C)\n",
    "            # focus only on the last time step \n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next =  torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append the sampled index to the running sequence \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel()\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "# print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "# Obviously this is gonna be gibberish because we haven't trained our model yet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer \n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    #sample a batch of training data \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss \n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "# TODO learn how to properly record the losses and trends with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "H's my and mere.\n",
      "\n",
      "Sechang dray honad,--\n",
      "And throng,\n",
      "Bugh we'll noo,\n",
      "Nurreeing on broint the light time,\n",
      "And race oven th's farering ethiany thy vity putle by any all in was no thraight us istorom the bust mastorn anture this bestrugh Yorthe Mour upon said dombny. Must way is my lades no complanzed\n",
      "Wach there thenge,\n",
      "Anded befe; 'Twimenjeckion.\n",
      "\n",
      "ANGELO:\n",
      "That bore nigh the queestank bust is thy you, would then\n",
      "commanus grif so six''.\n",
      "\n",
      "GRATHAM:\n",
      "My rues oth say thy friend for like there;\n",
      "And we to-ming were would are time?\n",
      "\n",
      "LERRT:\n",
      "Ay commind thy calousen are as night may ger brods\n",
      "Comes honound fight,\n",
      "unctis.\n",
      "\n",
      "WERK:\n",
      "Some, play OF WARGAMNE:\n",
      "Why lieve queest shought the zell livenk at both,\n",
      "Duke: paind Gloh outteng of sheas on goison.\n",
      "\n",
      "Sy gooldfes this a king to may mittell, 'the sech, no.\n",
      "\n",
      "MERCUTINIUS:\n",
      "O ploor Henrys laster'd haspech\n",
      "matimy Roughdams, 'tis have\n",
      "Siddrain proses arm thrate hisee,\n",
      "One.\n",
      "\n",
      "ANTENBELLO:\n",
      "Yet moves: in you of Menrad.\n",
      "\n",
      "First trotal their to skeed his you; bear be for \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mathematical trick in self attention\n",
    "torch.manual_seed(42)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C) # Batch, time, channels \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xbow[b, t] is the average of all the tokens before t in batch b \n",
    "xbow = torch.zeros(B, T, C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 0.9007, -2.1055],\n",
       "        [ 0.6784, -1.2345],\n",
       "        [-0.0431, -1.6047],\n",
       "        [-0.7521,  1.6487],\n",
       "        [-0.3925, -1.4036],\n",
       "        [-0.7279, -0.5594],\n",
       "        [-0.7688,  0.7624]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873],\n",
       "        [ 1.4138, -0.3091],\n",
       "        [ 1.1687, -0.6176],\n",
       "        [ 0.8657, -0.8644],\n",
       "        [ 0.5422, -0.3617],\n",
       "        [ 0.3864, -0.5354],\n",
       "        [ 0.2272, -0.5388],\n",
       "        [ 0.1027, -0.3762]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matmul trick is using lower triangular matrix instead of ones so that we can ignore the tokens following the current token in the computation process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      " tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "-------\n",
      "b = \n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "-------\n",
      "c = \n",
      " tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a@b\n",
    "\n",
    "print(\"a = \\n\", a)\n",
    "print(\"-------\")\n",
    "print(\"b = \\n\", b)\n",
    "print(\"-------\")\n",
    "print(\"c = \\n\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets apply this trick to our xbow computation \n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use softmax \n",
    "# The reason to do this is we are trying to formulate self attention \n",
    "# tril makes sure only the past tokens factor in the context of current token\n",
    "# wei initialized to 0 here is obtained from the data to record the affinity between the tokens \n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7629e+00, -1.3011e+00,  5.6516e-01,  2.1616e+00, -1.0674e+00,\n",
      "           1.9632e+00,  1.0765e+00, -4.5295e-01],\n",
      "         [-3.3334e+00, -1.6556e+00,  1.0405e-01,  3.3782e+00, -2.1825e+00,\n",
      "           1.0415e+00, -5.5714e-02,  2.9273e-01],\n",
      "         [-1.0226e+00, -1.2606e+00,  7.6228e-02, -3.8125e-01, -9.8430e-01,\n",
      "          -1.4303e+00,  7.4921e-02, -9.5465e-01],\n",
      "         [ 7.8359e-01, -8.0143e-01, -3.3680e-01, -8.4963e-01, -5.6023e-01,\n",
      "          -1.1701e+00, -1.2927e+00, -1.0260e+00],\n",
      "         [-1.2566e+00,  1.8719e-02, -7.8797e-01, -1.3204e+00,  2.0363e+00,\n",
      "           8.6381e-01,  3.7188e-01,  9.2577e-01],\n",
      "         [-3.1262e-01,  2.4152e+00, -1.1059e-01, -9.9305e-01,  3.3449e+00,\n",
      "          -2.5229e+00,  1.4187e+00,  1.2196e+00],\n",
      "         [ 1.0876e+00,  1.9652e+00, -2.6213e-01, -3.1579e-01,  6.0905e-01,\n",
      "           1.2616e+00, -5.4841e-01,  8.0485e-01],\n",
      "         [-1.8044e+00, -4.1260e-01, -8.3061e-01,  5.8985e-01, -7.9869e-01,\n",
      "          -5.8560e-01,  6.4332e-01,  6.3028e-01]],\n",
      "\n",
      "        [[-7.3529e-01, -1.7807e+00,  1.0745e+00, -2.7429e-01,  1.6347e+00,\n",
      "           1.4177e+00, -5.5213e-01, -2.3580e+00],\n",
      "         [-3.0892e+00, -1.4943e+00, -2.6167e-01,  2.2760e+00, -2.4364e-01,\n",
      "           1.6198e-01,  2.5783e+00,  3.9591e-01],\n",
      "         [-5.0206e-01, -2.0745e+00,  5.3785e-01, -4.0494e-01,  8.3292e-01,\n",
      "           1.3570e+00, -1.5621e+00, -1.6490e+00],\n",
      "         [ 1.3810e+00, -1.4713e-01,  1.2181e+00, -2.2266e-01, -1.8247e+00,\n",
      "          -3.7044e+00, -2.1321e+00,  1.3178e+00],\n",
      "         [-2.3568e+00, -4.6170e-01, -8.8196e-01,  2.3700e+00,  6.7829e-01,\n",
      "           1.6262e-01,  1.9379e+00,  1.0397e-01],\n",
      "         [-9.2435e-01, -6.2351e-01, -1.3938e+00,  1.3336e+00, -8.9725e-03,\n",
      "          -3.1789e+00,  9.0259e-01,  3.6256e+00],\n",
      "         [-6.5522e-01,  1.0991e+00, -2.1399e+00,  9.6468e-01,  9.9463e-01,\n",
      "           9.3899e-01,  4.6799e-01, -3.5870e-01],\n",
      "         [ 1.5463e+00, -4.9438e-01, -1.4181e-02, -9.7428e-01,  1.3779e+00,\n",
      "           7.8653e-03, -5.3590e-01, -4.5531e-01]],\n",
      "\n",
      "        [[-3.7898e-01,  5.1592e-01,  3.0332e-01,  1.1303e+00,  2.0511e+00,\n",
      "           2.2323e+00,  3.1239e+00, -1.2231e+00],\n",
      "         [ 1.0377e-01,  1.7584e-01, -1.6369e-01,  5.2328e-01, -2.2172e+00,\n",
      "          -8.7770e-01,  1.7020e-01, -1.0842e+00],\n",
      "         [-1.6373e+00, -6.5556e-01, -8.5031e-01,  2.3457e+00, -9.9497e-01,\n",
      "          -4.9228e-02,  5.5157e-01,  1.5285e+00],\n",
      "         [-2.7155e+00,  1.9022e+00, -8.4620e-01,  5.9058e-01,  2.1122e+00,\n",
      "           8.8971e-01, -2.0679e+00, -7.4249e-01],\n",
      "         [ 2.5044e+00, -4.9691e-01, -2.6300e-01, -1.6288e-01, -1.7459e+00,\n",
      "           8.6298e-02,  2.7739e+00, -2.4952e-02],\n",
      "         [-4.8634e-02,  4.9620e-01, -2.0859e-01, -8.4632e-02,  3.6811e-01,\n",
      "           7.8713e-01, -1.9678e-01,  4.1090e-01],\n",
      "         [-1.7485e+00,  4.6233e-01,  3.8657e-03,  2.1114e+00,  1.2731e+00,\n",
      "           2.1582e+00,  1.3125e+00,  2.0600e+00],\n",
      "         [-8.5500e-02, -1.5413e-02, -1.3915e+00,  6.3086e-02, -2.4530e-01,\n",
      "          -2.0677e-01, -2.2102e+00,  4.4531e-01]],\n",
      "\n",
      "        [[ 4.5165e-01,  3.2148e-01, -3.1926e+00,  3.0765e-01, -6.1612e-01,\n",
      "           2.5626e-01, -2.9891e-01, -2.1917e+00],\n",
      "         [-4.0009e-01, -9.6206e-01,  1.9568e+00,  6.6612e-01, -3.2630e-01,\n",
      "           2.6258e-01, -1.3973e+00, -8.9450e-01],\n",
      "         [-4.6199e-01,  5.8600e-01, -4.6738e+00, -3.2178e-01,  1.2684e+00,\n",
      "          -1.7402e-01,  1.2461e+00, -2.2283e+00],\n",
      "         [-7.1746e-01, -1.0279e+00, -2.0509e+00, -2.7234e+00,  3.1231e-01,\n",
      "          -1.6416e-01,  1.5162e+00, -7.7669e-01],\n",
      "         [-4.0388e-01,  5.1597e-01, -2.0697e+00, -4.0982e-01, -8.0534e-01,\n",
      "           5.2210e-01, -4.1242e-01,  1.3377e+00],\n",
      "         [ 8.2322e-01,  3.0237e+00, -3.0655e+00,  7.0404e-01,  6.7207e-01,\n",
      "          -4.6692e-01,  2.3746e+00,  3.1181e-01],\n",
      "         [-1.4141e+00, -1.4241e+00, -8.0387e-01, -1.7450e+00, -7.4035e-01,\n",
      "           9.8188e-01, -9.0056e-01, -2.3158e+00],\n",
      "         [-5.0277e-01,  1.6844e+00, -4.1847e-01,  1.0239e+00,  1.0275e+00,\n",
      "           1.3980e-01,  4.8822e-01,  1.5573e+00]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single head self-attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = torch.nn.Linear(C, head_size, bias=False)\n",
    "query = torch.nn.Linear(C, head_size, bias=False)\n",
    "value = torch.nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) x (B, 16, T) -> (B, T, T)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "out.shape\n",
    "\n",
    "# Note 1:\n",
    "# Attention is simply a communication network. In a strictly recurrent network, \n",
    "# there are edges between consecutive tokens only. In self-attention, we expand this \n",
    "# to allow any tokens to have information flow from any of the past tokens. \n",
    "# What's better, we weigh the edges by the importance each token should be getting from each of the previous tokens \n",
    "# Note 2:\n",
    "# There is no notion of space. the tokens themselves contain no information on where they are in a sequence, sentence, or paragraph.\n",
    "# That's why we need to add positional embedding with token embeddings at the beginning.\n",
    "# Note 3:\n",
    "# There is no communication along batch dimension even though they are computed together \n",
    "# Does batchnorm change this????\n",
    "# Note 4:\n",
    "# In *encoder* simply change tril to ones to allow tokens from *future* to influence the token in the past\n",
    "# thereby enabling the bidirectional communication. \n",
    "# Note 5:\n",
    "# The self-attention just means keys, queries, and values come from the same source\n",
    "# In enc-dec transformers queries comes from x, but keys and values come from whole different source, sometimes from the output of encoder blocks \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Residual connections "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
